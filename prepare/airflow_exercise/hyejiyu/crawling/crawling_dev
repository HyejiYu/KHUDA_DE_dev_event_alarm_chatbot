import pandas as pd
import requests
import re
from bs4 import BeautifulSoup as bs
import datetime
import time

def get_urls(url):
    response = requests.get(url)
    html = response.text
    soup = bs(html, 'html.parser')
    return soup

def get_hashtag(soup):
    hashtags_all = soup.find_all('div', class_='Item_tags___ujeV')
    hashtags  = []
    for hash in hashtags_all:
        txt = hash.text.split("# ")
        hashtags.append(txt[1:])
    return hashtags

def get_title(soup):
    title_all = soup.find_all('span', class_='Item_item__content__title___fPQa')
    titles = []
    for title in title_all:
        titles.append(title.text)
    return titles

def get_host(soup):
    hosts_all = soup.find_all('div', class_='Item_host__zNXMy')
    hosts = []
    for host in hosts_all:
        hosts.append(host.text)
    return hosts

def get_date(soup):
    dates_all = soup.find_all('div', class_='Item_date__kVMJZ')
    dates = []
    for date in dates_all:
        txt = re.sub(r"\s", "", date.text)
        txt = re.sub(r"\(.\)", "", txt)
        dates.append(txt.split("~"))
    return dates

def get_image(soup):
    images_all = soup.find_all('img', alt="/default/event_img.png")
    images = []
    for i, image in enumerate(images_all):
        if i % 2 == 1:
            text = 'https://dev-event.vercel.app'
            text += image.get('src')
            images.append(text)
    return images

def get_link(soup):
    link_all = soup.find_all('div', class_='Item_item__86e_I')
    links = []
    for link in link_all:
        txt = link.a.attrs["href"]
        links.append(txt)
    return links

def crawling(**context):
    soup = get_urls("https://dev-event.vercel.app/events")
    titles = get_title(soup)
    hosts = get_host(soup)
    hashtags = get_hashtag(soup)
    dates = get_date(soup)
    images = get_image(soup)
    links = get_link(soup)

    data = pd.DataFrame({'title': titles, 'host': hosts, 'hashtag': hashtags, \
                         'date': dates, 'image': images, 'link': links})
    date = datetime.datetime.now().strftime("%Y%m%d")
    data.to_csv(f'/home/ubuntu/airflow/data/dev-crawler-{date}.csv', index=False)
    context['task_instance'].xcom_push(key=f'dev-crawler-csv', value=f'/home/ubuntu/airflow/data/dev-crawler-{date}.csv')
